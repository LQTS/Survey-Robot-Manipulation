# 论文清单

目前的一些工作：[lqts.github.io](http://lqts.github.io)

- 抓取
    - UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy
    - UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning
    - Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction
    - **NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model**
- 遥操作
    - AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System
- 预训练
    - R3M: A Universal Visual Representation for Robot Manipulation
    - Real-World Robot Learning with Masked Visual Pre-training
    - The Unsurprising Effectiveness of Pre-Trained Vision Models for Control
    - LIV: Language-Image Representations and Rewards for Robotic Control
    - VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training
    - Language-Driven Representation Learning for Robotics
    - Any-point Trajectory Modeling for Policy Learning
    - Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play
    - Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling
- 灵巧操作
    - DexArt: Benchmarking Generalizable Dexterous Manipulation With Articulated Objects
    - A Multi-Agent Approach for Adaptive Finger Cooperation in Learning-based In-Hand Manipulation
    - A Multi-Agent Approach for Adaptive Finger Cooperation in Learning-based In-Hand Manipulation
    - DexPBT: Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training
    - Toward Human-Like Grasp: Functional Grasp by Dexterous Robotic Hand Via Object-Hand Semantic Representation
    - DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality
    - Robot Synesthesia: In-Hand Manipulation with Visuotactile Sensing
    - Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning
    - A System for General In-Hand Object Re-Orientation
    - General In-Hand Object Rotation with Vision and Touch
    - Visual Dexterity: In-Hand Reorientation of Novel and Complex Object Shapes
    - SimPLE, a visuotactile method learned in simulation to precisely pick, localize, regrasp, and place objects
- 长序列
    - Hierarchical reinforcement learning for in-hand robotic manipulation using Davenport chained rotations
    - Learning Hierarchical Control for Robust In-Hand Manipulation
    - Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation
- 视触任务
    - 3D Shape Reconstruction from Vision and Touch
    - Active 3D Shape Reconstruction from Vision and Touch
    - Visual-Tactile Sensing for In-Hand Object Reconstruction
    - VisuoTactile 6D Pose Estimation of an In-Hand Object Using Vision and Tactile Sensor Data
    - Hierarchical Graph Neural Networks for Proprioceptive 6D Pose Estimation of In-hand Objects
- 操作大模型
    - **Language-Image Goal-Conditioned Value Learning**
        - SayCan: Do As I Can, Not As I Say: Grounding Language in Robotic Affordances [[Paper]](https://arxiv.org/abs/2204.01691)[[Project]](https://say-can.github.io/)[[Code]](https://github.com/google-research/google-research/tree/master/saycan)
        - Zero-Shot Reward Specification via Grounded Natural Language [[Paper]](https://proceedings.mlr.press/v162/mahmoudieh22a/mahmoudieh22a.pdf)
        - VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models [[Project]](https://voxposer.github.io/)
        - VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training [[Paper]](https://arxiv.org/abs/2210.00030)[[Project]](https://sites.google.com/view/vip-rl)
        - LIV: Language-Image Representations and Rewards for Robotic Control [[Paper]](https://arxiv.org/abs/2306.00958)[[Project]](https://penn-pal-lab.github.io/LIV/)
        - LOReL: Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation [[Paper]](https://arxiv.org/abs/2109.01115)[[Project]](https://sites.google.com/view/robotlorel)
        - Text2Motion: From Natural Language Instructions to Feasible Plans [[Paper]](https://arxiv.org/abs/2303.12153)[[Project]](https://sites.google.com/stanford.edu/text2motion)
        - MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control [[Paper]](https://arxiv.org/pdf/2403.12037.pdf)[[Project]](https://sites.google.com/view/minedreamer/main)[[Code]](https://github.com/Zhoues/MineDreamer)
    - **Robot Transformers**
        - MotionGPT: Finetuned LLMs are General-Purpose Motion Generators [[Paper]](https://arxiv.org/abs/2306.10900)[[Project]](https://qiqiapink.github.io/MotionGPT/)
        - RT-1: Robotics Transformer for Real-World Control at Scale [[Paper]](https://robotics-transformer.github.io/assets/rt1.pdf)[[Project]](https://robotics-transformer.github.io/)[[Code]](https://github.com/google-research/robotics_transformer)
        - Masked Visual Pre-training for Motor Control [[Paper]](https://arxiv.org/abs/2203.06173)[[Project]](https://tetexiao.com/projects/mvp)[[Code]](https://github.com/ir413/mvp)
        - Real-world robot learning with masked visual pre-training [[Paper]](https://arxiv.org/abs/2210.03109)[[Project]](https://tetexiao.com/projects/real-mvp)
        - R3M: A Universal Visual Representation for Robot Manipulation [[Paper]](https://arxiv.org/abs/2203.12601)[[Project]](https://sites.google.com/view/robot-r3m/)[[Code]](https://github.com/facebookresearch/r3m)
        - Robot Learning with Sensorimotor Pre-training [[Paper]](https://arxiv.org/abs/2306.10007)[[Project]](https://robotic-pretrained-transformer.github.io/)
        - Rt-2: Vision-language-action models transfer web knowledge to robotic control [[Paper]](https://arxiv.org/abs/2307.15818)[[Project]](https://robotics-transformer2.github.io/)
        - PACT: Perception-Action Causal Transformer for Autoregressive Robotics Pre-Training [[Paper]](https://arxiv.org/abs/2209.11133)
        - GROOT: Learning to Follow Instructions by Watching Gameplay Videos [[Paper]](https://arxiv.org/pdf/2310.08235.pdf)[[Project]](https://craftjarvis.github.io/GROOT/)[[Code]](https://github.com/CraftJarvis/GROOT)
        - Behavior Transformers (BeT): Cloning k modes with one stone [[Paper]](https://arxiv.org/abs/2206.11251)[[Project]](https://mahis.life/bet/)[[Code]](https://github.com/notmahi/bet)
        - Conditional Behavior Transformers (C-BeT), From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data [[Paper]](https://arxiv.org/abs/2210.10047)[[Project]](https://play-to-policy.github.io/)[[Code]](https://github.com/jeffacce/play-to-policy)
        - MAGIC: Meta-learning Adaptation for Ground Interaction Control with Visual Foundation Models [[Paper]](https://arxiv.org/abs/2407.12304)
    
- 操作数据集
    - 人手操作数据集
        
        
        | GTEA-GAZE+ | http://gtubicomp2015grad.pbworks.com/w/file/fetch/94746323/fathi-recognize-daily-actions-using-gaze2012.pdf |
        | --- | --- |
        | EPIC-KITCHEN-100 | https://arxiv.org/pdf/2006.13256v4 |
        | FPHA | https://openaccess.thecvf.com/content_cvpr_2018/papers/Garcia-Hernando_First-Person_Hand_Action_CVPR_2018_paper.pdf |
        | GRAB | https://arxiv.org/pdf/2008.11200 |
        | ContactPose | https://arxiv.org/pdf/2007.09545 |
        | HO-3D | https://arxiv.org/abs/1907.01481 |
        | DexYCB | https://dex-ycb.github.io/assets/chao_cvpr2021.pdf |
        | H2O | https://arxiv.org/pdf/2104.11181 |
        | HOI4D | https://ieeexplore.ieee.org/document/9878670/ |
        | Ego4D | https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9879279 |
    - 机器人操作数据集
        
        
        | MIME | https://sites.google.com/view/mimedataset/dataset |
        | --- | --- |
        | ROBOTURK | https://arxiv.org/pdf/1811.02790 |
        | RoboNet | https://arxiv.org/pdf/1910.11215 |
        | Bridge Data | https://arxiv.org/pdf/2109.13396 |
        | BC-Z | https://arxiv.org/pdf/2202.02005 |
        | RT-1 | https://arxiv.org/pdf/2212.06817 |
        | RoboSet | https://arxiv.org/pdf/2309.01918 |
        | Bridge Data v2 | https://arxiv.org/pdf/2308.12952 |
        | RH20T | https://arxiv.org/pdf/2307.00595 |
        | RealDex | https://arxiv.org/pdf/2402.13853 |
        | DROID | https://arxiv.org/abs/2403.12945 |
